<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    

    <title>
      机器学习(coursera 斯坦福)第六周编程作业 | Sail 
    </title>

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    
      <meta name="author" content="Sail">


    
    

<div id="site_search">
            <input type="text" id="local-search-input" name="q" results="0" placeholder="search my blog..." class="form-control">
            <div id="local-search-result"></div>
    </div>
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-1345496474366685",
        enable_page_level_ads: true
      });
    </script>


    <meta name="description" content="Preface 本文是机器学习第六周部分课程内容,以及编程作业答案记录,完整题目以及代码见github   作业  linearRegCostFunction.m 12345678910111213141516171819202122232425262728293031function [J, grad] = linearRegCostFunction(X, y, theta, lambda)%">
<meta name="keywords" content="Advice for Applying Machine Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习(coursera 斯坦福)第六周编程作业 | Sail">
<meta property="og:url" content="http://www.sail.name/2018/06/07/the-sixth-week-homework-of-ML-Stanford/index.html">
<meta property="og:site_name" content="Sail">
<meta property="og:description" content="Preface 本文是机器学习第六周部分课程内容,以及编程作业答案记录,完整题目以及代码见github   作业  linearRegCostFunction.m 12345678910111213141516171819202122232425262728293031function [J, grad] = linearRegCostFunction(X, y, theta, lambda)%">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://www.sail.name/img/机器学习/the-sixth-week-homework-of-ML-Stanford/1.png">
<meta property="og:image" content="http://www.sail.name/img/机器学习/the-sixth-week-homework-of-ML-Stanford/2.png">
<meta property="og:image" content="http://www.sail.name/img/机器学习/the-sixth-week-homework-of-ML-Stanford/3.png">
<meta property="og:updated_time" content="2018-12-09T10:19:58.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习(coursera 斯坦福)第六周编程作业 | Sail">
<meta name="twitter:description" content="Preface 本文是机器学习第六周部分课程内容,以及编程作业答案记录,完整题目以及代码见github   作业  linearRegCostFunction.m 12345678910111213141516171819202122232425262728293031function [J, grad] = linearRegCostFunction(X, y, theta, lambda)%">
<meta name="twitter:image" content="http://www.sail.name/img/机器学习/the-sixth-week-homework-of-ML-Stanford/1.png">
    
    
    
      <link rel="icon" type="image/x-icon" href="/favicon.png">
    
    <link rel="stylesheet" href="/css/uno.css">
    <link rel="stylesheet" href="/css/highlight.css">
    <link rel="stylesheet" href="/css/archive.css">
    <link rel="stylesheet" href="/css/china-social-icon.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body>

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><span class="mobile btn-mobile-menu">
        <i class="icon icon-list btn-mobile-menu__icon"></i>
        <i class="icon icon-x-circle btn-mobile-close__icon hidden"></i>

    </span>

    

<header class="panel-cover panel-cover--collapsed">


  <div class="panel-main">

  
    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        

        <h1 class="panel-cover__title panel-title"><a href="/" title="link to homepage">Sail</a></h1>
        <hr class="panel-cover__divider">

        
        <p class="panel-cover__description">
          手在键盘敲很轻
        </p>
        <hr class="panel-cover__divider panel-cover__divider--secondary">
        

        <div class="navigation-wrapper">

          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">

              
                
                <li class="navigation__item"><a href="/#blog" title="" class="blog-button">首页</a></li>

              
                
                <li class="navigation__item"><a href="/about" title="" class="">关于</a></li>

              
                
                <li class="navigation__item"><a href="/archive" title="" class="">归档</a></li>

              
               <li class="navigation__item">
                    <a href="http://www.sail.name/Resource/" title="个人资源分享">Resource</a></li>

            </ul>
          </nav>
          <!-- ----------------------------
To add a new social icon simply duplicate one of the list items from below
and change the class in the <i> tag to match the desired social network
and then add your link to the <a>. Here is a full list of social network
classes that you can use:

    icon-social-500px
    icon-social-behance
    icon-social-delicious
    icon-social-designer-news
    icon-social-deviant-art
    icon-social-digg
    icon-social-dribbble
    icon-social-facebook
    icon-social-flickr
    icon-social-forrst
    icon-social-foursquare
    icon-social-github
    icon-social-google-plus
    icon-social-hi5
    icon-social-instagram
    icon-social-lastfm
    icon-social-linkedin
    icon-social-medium
    icon-social-myspace
    icon-social-path
    icon-social-pinterest
    icon-social-rdio
    icon-social-reddit
    icon-social-skype
    icon-social-spotify
    icon-social-stack-overflow
    icon-social-steam
    icon-social-stumbleupon
    icon-social-treehouse
    icon-social-tumblr
    icon-social-twitter
    icon-social-vimeo
    icon-social-xbox
    icon-social-yelp
    icon-social-youtube
    icon-social-zerply
    icon-mail

-------------------------------->

<!-- add social info here -->



<nav class="cover-navigation navigation--social">
  <ul class="navigation">
    
      <!-- Github -->
      <li class="navigation__item">
        <a href="https://github.com/iamsail" title="Huno on GitHub">
          <i class="icon icon-social-github"></i>
          <span class="label">GitHub</span>
        </a>
      </li>
    

    <!-- China social icon -->
    <!--
    
      <li class="navigation__item">
        <a href="" title="">
          <i class='icon cs-icon-douban'></i>
          <span class="label">Douban</span>
        </a>
      </li>

      <li class="navigation__item">
        <a href="" title="">
          <i class='icon cs-icon-weibo'></i>
          <span class="label">Weibo</span>
        </a>
      </li>

    -->



  </ul>
</nav>


        </div>

      </div>

    </div>

    <div class="panel-cover--overlay"></div>
  </div>
</header>

    <div class="content-wrapper">
        <div class="content-wrapper__inner entry">
            

<article class="post-container post-container--single">

  <header class="post-header">
    
    <h1 class="post-title">机器学习(coursera 斯坦福)第六周编程作业</h1>

    

    <div class="post-meta">
      <time datetime="2018-06-07" class="post-meta__date date">2018-06-07</time> 

      <span class="post-meta__tags tags">

          
            <font class="categories">
            &#8226; 分类:
            <a class="categories-link" href="/categories/机器学习/">机器学习</a>
            </font>
          

          
             &#8226; 标签:
            <font class="tags">
              <a class="tags-link" href="/tags/Advice-for-Applying-Machine-Learning/">Advice for Applying Machine Learning</a>
            </font>
          

      </span>
    </div>
    
    

  </header>

  <section id="post-content" class="article-content post">
    <h3 id="Preface"><a href="#Preface" class="headerlink" title=" Preface "></a><strong> Preface </strong></h3><p>本文是机器学习第六周部分课程内容,以及编程作业答案记录,完整题目以及代码见<a href="https://github.com/iamsail/ML-Stanford/tree/master/homework/fifth-week" target="_blank" rel="noopener">github</a></p>
<hr>
<h3 id="作业"><a href="#作业" class="headerlink" title=" 作业 "></a><strong> 作业 </strong></h3><h4 id="linearRegCostFunction-m"><a href="#linearRegCostFunction-m" class="headerlink" title=" linearRegCostFunction.m "></a><strong> linearRegCostFunction.m </strong></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">function [J, grad] = linearRegCostFunction(X, y, theta, lambda)</span><br><span class="line">%LINEARREGCOSTFUNCTION Compute cost and gradient for regularized linear </span><br><span class="line">%regression with multiple variables</span><br><span class="line">%   [J, grad] = LINEARREGCOSTFUNCTION(X, y, theta, lambda) computes the </span><br><span class="line">%   cost of using theta as the parameter for linear regression to fit the </span><br><span class="line">%   data points in X and y. Returns the cost in J and the gradient in grad</span><br><span class="line"></span><br><span class="line">% Initialize some useful values</span><br><span class="line">m = length(y); % number of training examples</span><br><span class="line"></span><br><span class="line">% You need to return the following variables correctly </span><br><span class="line">J = 0;</span><br><span class="line">grad = zeros(size(theta));</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Compute the cost and gradient of regularized linear </span><br><span class="line">%               regression for a particular choice of theta.</span><br><span class="line">%</span><br><span class="line">%               You should set J to the cost and grad to the gradient.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">J = 1 / (2 * m) * sum((X * theta -y) .^ 2) + lambda / (2 * m) * sum(theta(2:end).^ 2);</span><br><span class="line"></span><br><span class="line">grad = 1 / m * (X&apos; * (X * theta - y));</span><br><span class="line">grad(2:end) = grad(2:end) + lambda / m * theta(2:end);</span><br><span class="line"></span><br><span class="line">% =========================================================================</span><br><span class="line"></span><br><span class="line">grad = grad(:);</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="learningCurve-m"><a href="#learningCurve-m" class="headerlink" title=" learningCurve.m "></a><strong> learningCurve.m </strong></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">function [error_train, error_val] = ...</span><br><span class="line">    learningCurve(X, y, Xval, yval, lambda)</span><br><span class="line">%LEARNINGCURVE Generates the train and cross validation set errors needed </span><br><span class="line">%to plot a learning curve</span><br><span class="line">%   [error_train, error_val] = ...</span><br><span class="line">%       LEARNINGCURVE(X, y, Xval, yval, lambda) returns the train and</span><br><span class="line">%       cross validation set errors for a learning curve. In particular, </span><br><span class="line">%       it returns two vectors of the same length - error_train and </span><br><span class="line">%       error_val. Then, error_train(i) contains the training error for</span><br><span class="line">%       i examples (and similarly for error_val(i)).</span><br><span class="line">%</span><br><span class="line">%   In this function, you will compute the train and test errors for</span><br><span class="line">%   dataset sizes from 1 up to m. In practice, when working with larger</span><br><span class="line">%   datasets, you might want to do this in larger intervals.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Number of training examples</span><br><span class="line">m = size(X, 1);</span><br><span class="line"></span><br><span class="line">% You need to return these values correctly</span><br><span class="line">error_train = zeros(m, 1);</span><br><span class="line">error_val   = zeros(m, 1);</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Fill in this function to return training errors in </span><br><span class="line">%               error_train and the cross validation errors in error_val. </span><br><span class="line">%               i.e., error_train(i) and </span><br><span class="line">%               error_val(i) should give you the errors</span><br><span class="line">%               obtained after training on i examples.</span><br><span class="line">%</span><br><span class="line">% Note: You should evaluate the training error on the first i training</span><br><span class="line">%       examples (i.e., X(1:i, :) and y(1:i)).</span><br><span class="line">%</span><br><span class="line">%       For the cross-validation error, you should instead evaluate on</span><br><span class="line">%       the _entire_ cross validation set (Xval and yval).</span><br><span class="line">%</span><br><span class="line">% Note: If you are using your cost function (linearRegCostFunction)</span><br><span class="line">%       to compute the training and cross validation error, you should </span><br><span class="line">%       call the function with the lambda argument set to 0. </span><br><span class="line">%       Do note that you will still need to use lambda when running</span><br><span class="line">%       the training to obtain the theta parameters.</span><br><span class="line">%</span><br><span class="line">% Hint: You can loop over the examples with the following:</span><br><span class="line">%</span><br><span class="line">%       for i = 1:m</span><br><span class="line">%           % Compute train/cross validation errors using training examples </span><br><span class="line">%           % X(1:i, :) and y(1:i), storing the result in </span><br><span class="line">%           % error_train(i) and error_val(i)</span><br><span class="line">%           ....</span><br><span class="line">%           </span><br><span class="line">%       end</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% ---------------------- Sample Solution ----------------------</span><br><span class="line"></span><br><span class="line">for i = 1:m</span><br><span class="line">    theta = trainLinearReg(X(1:i,:), y(1:i), lambda);</span><br><span class="line">    error_train(i) = linearRegCostFunction(X(1:i,:), y(1:i), theta, 0);</span><br><span class="line">    error_val(i) = linearRegCostFunction(Xval, yval, theta, 0);</span><br><span class="line">end	 		</span><br><span class="line"></span><br><span class="line">% -------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">% =========================================================================</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="polyFeatures-m"><a href="#polyFeatures-m" class="headerlink" title=" polyFeatures.m "></a><strong> polyFeatures.m </strong></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">function [X_poly] = polyFeatures(X, p)</span><br><span class="line">%POLYFEATURES Maps X (1D vector) into the p-th power</span><br><span class="line">%   [X_poly] = POLYFEATURES(X, p) takes a data matrix X (size m x 1) and</span><br><span class="line">%   maps each example into its polynomial features where</span><br><span class="line">%   X_poly(i, :) = [X(i) X(i).^2 X(i).^3 ...  X(i).^p];</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% You need to return the following variables correctly.</span><br><span class="line">X_poly = zeros(numel(X), p);</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Given a vector X, return a matrix X_poly where the p-th </span><br><span class="line">%               column of X contains the values of X to the p-th power.</span><br><span class="line">%</span><br><span class="line">% </span><br><span class="line"></span><br><span class="line">for j = 1:p</span><br><span class="line"> X_poly(:,j) = X.^j;</span><br><span class="line">end</span><br><span class="line">% =========================================================================</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="validationCurve-m"><a href="#validationCurve-m" class="headerlink" title=" validationCurve.m "></a><strong> validationCurve.m </strong></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">function [lambda_vec, error_train, error_val] = ...</span><br><span class="line">    validationCurve(X, y, Xval, yval)</span><br><span class="line">%VALIDATIONCURVE Generate the train and validation errors needed to</span><br><span class="line">%plot a validation curve that we can use to select lambda</span><br><span class="line">%   [lambda_vec, error_train, error_val] = ...</span><br><span class="line">%       VALIDATIONCURVE(X, y, Xval, yval) returns the train</span><br><span class="line">%       and validation errors (in error_train, error_val)</span><br><span class="line">%       for different values of lambda. You are given the training set (X,</span><br><span class="line">%       y) and validation set (Xval, yval).</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Selected values of lambda (you should not change this)</span><br><span class="line">lambda_vec = [0 0.001 0.003 0.01 0.03 0.1 0.3 1 3 10]&apos;;</span><br><span class="line"></span><br><span class="line">% You need to return these variables correctly.</span><br><span class="line">error_train = zeros(length(lambda_vec), 1);</span><br><span class="line">error_val = zeros(length(lambda_vec), 1);</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Fill in this function to return training errors in </span><br><span class="line">%               error_train and the validation errors in error_val. The </span><br><span class="line">%               vector lambda_vec contains the different lambda parameters </span><br><span class="line">%               to use for each calculation of the errors, i.e, </span><br><span class="line">%               error_train(i), and error_val(i) should give </span><br><span class="line">%               you the errors obtained after training with </span><br><span class="line">%               lambda = lambda_vec(i)</span><br><span class="line">%</span><br><span class="line">% Note: You can loop over lambda_vec with the following:</span><br><span class="line">%</span><br><span class="line">%       for i = 1:length(lambda_vec)</span><br><span class="line">%           lambda = lambda_vec(i);</span><br><span class="line">%           % Compute train / val errors when training linear </span><br><span class="line">%           % regression with regularization parameter lambda</span><br><span class="line">%           % You should store the result in error_train(i)</span><br><span class="line">%           % and error_val(i)</span><br><span class="line">%           ....</span><br><span class="line">%           </span><br><span class="line">%       end</span><br><span class="line">%</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">for i=1:size(lambda_vec, 1)</span><br><span class="line">    theta = trainLinearReg(X, y, lambda_vec(i));</span><br><span class="line">    error_train(i) = linearRegCostFunction(X, y, theta, 0);</span><br><span class="line">    error_val(i) = linearRegCostFunction(Xval, yval, theta, 0);</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">% =========================================================================</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="课程部分内容记录"><a href="#课程部分内容记录" class="headerlink" title=" 课程部分内容记录 "></a><strong> 课程部分内容记录 </strong></h3><h4 id="Evaluating-a-Hypothesis"><a href="#Evaluating-a-Hypothesis" class="headerlink" title=" Evaluating a Hypothesis "></a><strong> Evaluating a Hypothesis </strong></h4><p>Once we have done some trouble shooting for errors in our predictions by:</p>
<blockquote>
<p>1.Getting more training examples<br>2.Trying smaller sets of features<br>3.Trying additional features<br>4.Trying polynomial features<br>5.Increasing or decreasing λ<br>6.We can move on to evaluate our new hypothesis.</p>
</blockquote>
<p>A hypothesis may have a low error for the training examples but still be inaccurate (because of overfitting). Thus, to evaluate a hypothesis, given a dataset of training examples, we can split up the data into two sets: a training set and a test set. <span class="under0"><strong> Typically, the training set consists of 70 % of your data and the test set is the remaining 30 %.</strong></span></p>
<hr>
<h4 id="Diagnosing-Bias-vs-Variance"><a href="#Diagnosing-Bias-vs-Variance" class="headerlink" title=" Diagnosing Bias vs. Variance "></a><strong> Diagnosing Bias vs. Variance </strong></h4><p><strong>High bias (underfitting): </strong>both $J_{train}(\Theta)$  and $J_{CV}(\Theta)$ will be high. Also, $J_{CV}(\Theta) \approx J_{train}(\Theta)$<br><strong> High variance (overfitting): </strong>$J_{train}(\Theta)$ will be low and $J_{CV}(\Theta)$ will be much greater than $J_{train}(\Theta)$<br>The is summarized in the figure below:</p>
<p><img src="/img/机器学习/the-sixth-week-homework-of-ML-Stanford/1.png" alt="1.png"></p>
<hr>
<h4 id="Learning-Curves"><a href="#Learning-Curves" class="headerlink" title=" Learning Curves "></a><strong> Learning Curves </strong></h4><p><img src="/img/机器学习/the-sixth-week-homework-of-ML-Stanford/2.png" alt="2.png"></p>
<hr>
<h4 id="Deciding-What-to-Do-Next-Revisited"><a href="#Deciding-What-to-Do-Next-Revisited" class="headerlink" title=" Deciding What to Do Next Revisited "></a><strong> Deciding What to Do Next Revisited </strong></h4><p>Our decision process can be broken down as follows:</p>
<blockquote>
<ol>
<li>Getting more training examples: Fixes high variance</li>
<li>Trying smaller sets of features: Fixes high variance</li>
<li>Adding features: Fixes high bias</li>
<li>Adding polynomial features: Fixes high bias</li>
<li>Decreasing λ: Fixes high bias</li>
<li>Increasing λ: Fixes high variance.</li>
</ol>
</blockquote>
<hr>
<p><img src="/img/机器学习/the-sixth-week-homework-of-ML-Stanford/3.png" alt="3.png"></p>
<hr>
<h3 id="参考"><a href="#参考" class="headerlink" title=" 参考 "></a><strong> 参考 </strong></h3><p><a href="https://blog.csdn.net/qq_27008079/article/details/72511079" target="_blank" rel="noopener">Coursera吴恩达机器学习课程 总结笔记及作业代码——第6周有关机器学习的小建议</a><br><a href="http://blog.sina.com.cn/s/blog_9e67285801010ttn.html" target="_blank" rel="noopener">matlab函数 bsxfun浅谈（转载）</a><br><a href="https://blog.csdn.net/hns20070/article/details/8926084" target="_blank" rel="noopener">Octave学习随笔（更新至6.16）【bsxfun】</a><br><a href="https://www.mathworks.com/help/matlab/ref/ldivide.html" target="_blank" rel="noopener">ldivide, .\</a></p>

  </section>

  
  
</article>


            <footer class="__share_wrapper">
    <span class="under0">
		<span class="text">关注我的微信公众号[<a href="/about/">李一二</a>]，即时看更多的文章</span>
	</span>
</footer>

<footer class="footer">
    <span class="footer__copyright">&copy; 2016-2021. | 由<a href="https://hexo.io/">Hexo</a>强力驱动 | 主题<a href="https://github.com/someus/huno">Huno</a> | <a href="https://beian.miit.gov.cn/">渝ICP备17002561号</a> | 不装弱了,我要做大佬</span>
</footer>

<script>
// 对特定页面的footer隐藏
// function find(str, aim, num) {
//    var resultIndex = str.indexOf(aim);
//    for(var i = 0; i < num - 1; i++){
//        resultIndex = str.indexOf(aim, resultIndex + 1);
//    }
//    return resultIndex;
//  }

// var __one = document.querySelector("#__one");
// var __tow = document.querySelector("#__tow");
// var __three = document.querySelector("#__three");
// var __threeIndex  = find(location.href, '/', 3);
// var __fourthIndex = find(location.href, '/', 4);
// var aimStr = '';

// if (__fourthIndex !== -1) {
//     aimStr = location.href.substring(__threeIndex + 1, __fourthIndex);
// } else if (__fourthIndex === -1) {
//     aimStr = location.href.substring(__threeIndex + 1);
// }

// switch (aimStr) {
//   case "":
//   case "#blog":
//   case "about":
//   case "archive":
//   case "categories":
//   case "tags":
//   case "page":
//     __one.style.display = "none";
//     __tow.style.display = "none";
//     __three.style.display = "none";
//     break;
// }
</script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-137711129-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-137711129-1');
</script>

        </div>
    </div>



    <!-- js files -->
    <script src="/js/jquery.min.js"></script>
    <script src="/js/main.js"></script>
    <script src="/js/scale.fix.js"></script>
    <script>

var searchFunc = function(path, search_id, content_id) {
    'use strict';
    $.ajax({
        url: path,
        dataType: "xml",
        success: function( xmlResponse ) {
            // get the contents from search data

            var datas = $( "entry", xmlResponse ).map(function() {
                return {
                    title: $( "title", this ).text(),
                    content: $("content",this).text(),
                    url: $( "link" , this).attr("href")
                };
            }).get();
            var $input = document.getElementById(search_id);
            var $resultContent = document.getElementById(content_id);
            $input.addEventListener('input', function(){
                var str='<ul class=\"search-result-list\">';
                var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                $resultContent.innerHTML = "";
                if (this.value.trim().length <= 0) {
                    return;
                }

                // perform local searching
                datas.forEach(function(data) {
                    var isMatch = true;
                    var content_index = [];
                    var data_title = data.title.trim().toLowerCase();
                    var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                    var data_url = data.url;
                    var index_title = -1;
                    var index_content = -1;
                    var first_occur = -1;
                    // only match artiles with not empty titles and contents
                    if(data_title != '' && data_content != '') {
                        keywords.forEach(function(keyword, i) {
                            index_title = data_title.indexOf(keyword);
                            index_content = data_content.indexOf(keyword);
                            if( index_title < 0 && index_content < 0 ){
                                isMatch = false;
                            } else {
                                if (index_content < 0) {
                                    index_content = 0;
                                }
                                if (i == 0) {
                                    first_occur = index_content;
                                }
                            }
                        });
                    }
                    // show search results
                    if (isMatch) {

                        str += "<li><a href='"+ data_url  +"' class='search-result-title'>"+ data_title +"</a>";
                       console.log(data_url);
                        console.log("=====命中=====", data);


                        var content = data.content.trim().replace(/<[^>]+>/g,"");
                        if (first_occur >= 0) {
                            // cut out 100 characters
                            var start = first_occur - 20;
                            var end = first_occur + 80;
                            if(start < 0){
                                start = 0;
                            }
                            if(start == 0){
                                end = 100;
                            }
                            if(end > content.length){
                                end = content.length;
                            }
                            var match_content = content.substr(start, end);
                            // highlight all keywords
                            keywords.forEach(function(keyword){
                                var regS = new RegExp(keyword, "gi");
                                match_content = match_content.replace(regS, "<em class=\"search-keyword\">"+keyword+"</em>");
                            });

                            str += "<p class=\"search-result\">" + match_content +"...</p>"
                        }
                        str += "</li>";
                    }
                });
                str += "</ul>";
                $resultContent.innerHTML = str;
            });
        }
    });
};

var path = "/" + "search.xml";
searchFunc(path, 'local-search-input', 'local-search-result');
    </script>


    

    <script src="/js/awesome-toc.min.js"></script>
    <script>
        $(document).ready(function(){
            $.awesome_toc({
                overlay: true,
                contentId: "post-content",
            });
        });
    </script>


    
    
    <!--kill ie6 -->
<!--[if IE 6]>
  <script src="//letskillie6.googlecode.com/svn/trunk/2/zh_CN.js"></script>
<![endif]-->

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>
